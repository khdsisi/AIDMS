{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STuxHh6kk3eL"
      },
      "source": [
        "# AI, Decision-Making and Society Problem Set #1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhT1u-Pof10V"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXeBvHPDdgjo"
      },
      "source": [
        "The goal of this homework is to provide some hands-on experience of exploratory data analysis and data curation. We will work step-by-step through investigating a dataset, training some ML models, and evaluating the models. We will apply a minimal version of the Grounded Theory approach to define custom categories and consider the effects of different design decisions on model performance. Overall, the homework is divided into 5 parts:\n",
        "\n",
        "1.   Spinning Up:  Prepare the environment and load the dataset.\n",
        "2.   Quantitative Evaluation: Train a simple model and evaluate its performance.\n",
        "3.   Exploratory Analysis: Analyze the dataset, improving it through manual cleaning and modifications.\n",
        "4.   Custom Training & Evaluation: Conduct prompt engineering to explore the capabilities of large language models.\n",
        "5.   Reflections on the Process: Reflect on the entire process and discuss the results.\n",
        "\n",
        "This notebook is designed to be run in a Google Colab environment.\n",
        "\n",
        "**Submission Instructions**: Please submit a PDF of your completed notebook to Gradescope. Make sure that all cell outputs are included. This assignment is due by 11:59 PM on Wednesday, 9/18/24.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0CvX7DlSYgF"
      },
      "source": [
        "## Spinning Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzolXrT6dgjp"
      },
      "source": [
        "In this section, we will set up our environment for the rest of the assignment. We will:\n",
        "\n",
        "*   Set up your colab to interact with the Gemini language model\n",
        "*   Download a dataset to use for the assignment\n",
        "*   Implement code to train a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6ndiw1TuOj"
      },
      "source": [
        "### Setting up your coding environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wWGPU4dgjq"
      },
      "source": [
        "First, we will set up the python environment to interact with libraries for model training and data manipulation.\n",
        "Moreover, make sure to enable the GPU on the notebook by navigating to the 'runtime' tab, then clicking on 'change runtime type' and then selecting one of the available GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXq0ygI3BCdQ"
      },
      "outputs": [],
      "source": [
        "## Install the generative AI interface\n",
        "!pip install -U -q google-generativeai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiJjB2vWCQJP"
      },
      "outputs": [],
      "source": [
        "## Imports in order to call relevant libraries\n",
        "import re\n",
        "import tqdm\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import layers\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import sklearn.metrics as skmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mwJYXpElYJc"
      },
      "source": [
        "Next, we will need to configure our code to connect to the language model server. You can do this with a Colab Secret named `GOOGLE_API_KEY`. If you don't already have this configured (or you're unsure if you do) follow the instructions in the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sleZny3T4cC"
      },
      "outputs": [],
      "source": [
        "API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5B9sWq0hNEV"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwFOAUsdgjs"
      },
      "source": [
        "Now that we have configured our environment, we need some data to get started. For this assignment, we will use the [Employee Review dataset](https://www.kaggle.com/datasets/fiodarryzhykau/employee-review/code). This is a dataset of 880 employee performance reviews with a combined metric that asseses the performance & potential of the employees, measured on a scale from 1 to 9. We will consider the potential use of AI systems trained on this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1AFbbOaH5jC"
      },
      "source": [
        "### Getting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi-_MvcHdgjt"
      },
      "source": [
        "Download the dataset from [Kaggle](https://www.kaggle.com/datasets/fiodarryzhykau/employee-review/code) and extract the csv files train_set.csv and test_set.csv to your local machine. Then, upload it to the Colab environment.\n",
        "\n",
        "You can find instructions for uploading a file to a colab session [here](https://saturncloud.io/blog/how-to-use-google-colab-to-work-with-local-files/#:~:text=Uploading%20Files%20to%20Google%20Colab&text=Click%20on%20the%20%E2%80%9CFiles%E2%80%9D%20tab,for%20the%20upload%20to%20complete.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3bhQtundgjt"
      },
      "source": [
        "## Question 1: Looking at the Data and Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWg_xcQTdgjt"
      },
      "source": [
        "Most data has documentation that describes how it was collected, what its intended purposes were, and known issues or risks. (If your data doesn't have this, its generally good practice to ask why!) Before we interact with the data, look at the documentation for this dataset and some example entries on [Kaggle](https://www.kaggle.com/datasets/fiodarryzhykau/employee-review/data). Answer the following questions with 1-3 sentences each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQYXiU5cWfXt"
      },
      "source": [
        "#### Q 1.1: How was this dataset constructed?\n",
        "\n",
        "---fill your answer here---\n",
        "\n",
        "#### Q 1.2: What was the original purpose for the dataset?\n",
        "\n",
        "---fill your answer here---\n",
        "\n",
        "#### Q 1.3: What does it mean that the dataset is \"partially reviewed\"? Why might this be important?\n",
        "\n",
        "---fill your answer here---\n",
        "\n",
        "#### Q 1.4: Identify and describe one potentially appropriate and one potentially inappropriate application of the dataset (or a model trained on it). These applications should be hypothetical and do not need to directly correspond to a specific real-world use case of this dataset.\n",
        "\n",
        "---fill your answer here---\n",
        "\n",
        "#### Related Reading\n",
        "\n",
        "[Datasheets for Datasets](https://arxiv.org/abs/1803.09010) articulates the motivation and reasoning behind this documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otclq8YIH5jD"
      },
      "source": [
        "Now that we've considered the data source, let's look at the data! We'll do that by reading the .csv files into a [pandas](https://pandas.pydata.org/) dataframe (using the command pd.read_csv) and display the first few rows of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDoKis4om-Ea"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train_set.csv')\n",
        "train_df.set_index('id', inplace=True)\n",
        "\n",
        "test_df = pd.read_csv('test_set.csv')\n",
        "test_df.set_index('id', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDz9MjkNl_FD"
      },
      "source": [
        "Run this cell to see an example of what a data point from the training set looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8YIc5Hqv90"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-WlKzXjYWn"
      },
      "source": [
        "## Question 2: Define and train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7I2b9wedgju"
      },
      "source": [
        "In this section we will take the feedback review and will generate the embeddings, from which we will try to predict the performance/potential score for each employee.\n",
        "The generation of the embedding is done similarly to the next text classification [example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Classify_text_with_embeddings.ipynb#scrollTo=_mwJYXpElYJc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPECMeE2xYA_"
      },
      "source": [
        "### Creating the embeddings\n",
        "\n",
        "Create the embeddings using the embedding-001 model, similarly to the provided classification example. Make sure that your colab notebook is enabled with the GPU configuration.\n",
        "\n",
        "###### Remark: the embedding generation might take a while, so be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTBGKkPQsotz"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from google.api_core import retry\n",
        "\n",
        "def make_embed_text_fn(model):\n",
        "\n",
        "  @retry.Retry(timeout=300.0)\n",
        "  def embed_fn(text: str) -> list[float]:\n",
        "    # Set the task_type to CLASSIFICATION.\n",
        "    embedding = genai.embed_content(model=model,\n",
        "                                    content=text,\n",
        "                                    task_type=\"classification\")\n",
        "    return embedding['embedding']\n",
        "\n",
        "  return embed_fn\n",
        "\n",
        "def create_embeddings(model, df):\n",
        "  df['Embeddings'] = df['feedback'].progress_apply(make_embed_text_fn(model))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH0yrHUHtHtw"
      },
      "outputs": [],
      "source": [
        "model_embeddings = 'models/embedding-001'\n",
        "df_train = create_embeddings(model_embeddings, train_df)\n",
        "df_test = create_embeddings(model_embeddings, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHNVHyE8bI-j"
      },
      "source": [
        "Now, if we look at the data we can see that there is a new column that includes the embeddings for the review. There are opaque numbers that show how a model represents a particular example. We will use these embeddings to train a custom classifer next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G5TvLlmRjHc"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdG7XTWplKoL"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPYEYkIsWt_5"
      },
      "source": [
        "## Question 3: Build a simple classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNoJMZdydgjv"
      },
      "source": [
        "Following the standard classifier example, we will now build a classifier comprised of two fully-connected layers, and will use it to classify the different performance/potential metric based on the feedback embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oLGi4w5JsQR"
      },
      "outputs": [],
      "source": [
        "def build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n",
        "  inputs = x = keras.Input(shape=(input_size,))\n",
        "  x = layers.Dense(input_size, activation='relu')(x)\n",
        "  x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "  return keras.Model(inputs=[inputs], outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kORA1Akl5GsG"
      },
      "outputs": [],
      "source": [
        "# Derive the embedding size from the first training element.\n",
        "embedding_size = len(df_train['Embeddings'].iloc[0])\n",
        "\n",
        "# Give your model a different name, as you have already used the variable name\n",
        "# 'model'\n",
        "classifier = build_classification_model(embedding_size,\n",
        "                                        len(df_train['label'].unique()))\n",
        "classifier.summary()\n",
        "\n",
        "classifier.compile(loss = keras.losses.SparseCategoricalCrossentropy(\n",
        "                          from_logits=True),\n",
        "                   optimizer = keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                   metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbpTGGiMXDxl"
      },
      "source": [
        "### Question 3.1: Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjKwYwU8dgjx"
      },
      "source": [
        "In order to train this model, we need to set the training data for our classifier. Modify the code below so that the training data are stored in the following variables:\n",
        "\n",
        "```\n",
        "y_train, x_train\n",
        "```\n",
        "and the validation data in\n",
        "```\n",
        "y_val, x_val\n",
        "```\n",
        "Note that, similarly to the reference [example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Classify_text_with_embeddings.ipynb#scrollTo=_mwJYXpElYJc), you might find it useful to use the function \"np.stack\" for the concatenation of the embeddings.\n",
        "\n",
        "\n",
        "Furthermore, feel free to try several different values of NUM_EPOCHS and BATCH_SIZE to see how they affect the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGgvMZGfJ1A4"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 40\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Configure the training data to fit the classifier.\n",
        "y_train = ## YOUR CODE TO EXTRACT TRAINING LABELS HERE\n",
        "x_train = ## YOUR CODE TO EXTRACT TRAINING INPUTS HERE\n",
        "y_val   = ## YOUR CODE TO EXTRACT VALIDATION LABELS HERE\n",
        "x_val   = ## YOUR CODE TO EXTRACT VALIDATION INPUTS HERE\n",
        "\n",
        "# Train the model for the desired number of epochs.\n",
        "history = classifier.fit(x=x_train,\n",
        "                         y=y_train,\n",
        "                         validation_data=(x_val, y_val),\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         epochs=NUM_EPOCHS,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGBaDHZUPdJO"
      },
      "source": [
        "### Question 3.2: Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFXiUAI0dgjy"
      },
      "source": [
        "We will use Keras' <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\"><code>Model.evaluate</code></a> procedure to get the loss and the accuracy on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2kOeiqqQIB8"
      },
      "outputs": [],
      "source": [
        "## Evaluate classifier on validation data\n",
        "classifier.evaluate(x=x_val, y=y_val, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5chvz-kq_uG"
      },
      "source": [
        "When evaluating model performance, we need to consider the possibility that our evaluation got lucky. One way to approach this is through the use of confidence intervals. These tell us a range of possible accuracy values that are consistent with the data (see a more detailed description in the the supportive PDF file). Calculate the 95% confidence interval for the accuracy of the model using the formula\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathrm{CI} = \\mathrm{accuracy} \\pm 1.96 \\times \\sqrt{\\frac{\\mathrm{accuracy} \\times (1 - \\mathrm{accuracy})}{\\mathrm{test \\ size}}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL5d6QKkrBdm"
      },
      "outputs": [],
      "source": [
        "def build_CI(y_hat, y_true):\n",
        "  # The function should return the one-sided width of the confidence interval,\n",
        "  # as it described in the equation above.\n",
        "  ## FILL IN WITH YOUR CODE TO COMPUTE THE CONFIDENCE INTERVALS WIDTH ##\n",
        "  return CI_onesided_width\n",
        "\n",
        "y_hat_test = classifier.predict(x=x_val)\n",
        "y_hat_test = np.argmax(y_hat_test, axis=1)\n",
        "print('========== CIs ==========')\n",
        "initial_CI_onesided_width = build_CI(y_hat_test, y_val)\n",
        "print(initial_CI_onesided_width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyxMhiLYQXAN"
      },
      "source": [
        "It is often useful to look at performance over the course of training. The next cell gives code to create this plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaDO9hwbEOW3"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "  \"\"\"\n",
        "    Plotting training and validation learning curves.\n",
        "\n",
        "    Args:\n",
        "      history: model history with all the metric measures\n",
        "  \"\"\"\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "  fig.set_size_inches(20, 8)\n",
        "\n",
        "  # Plot loss\n",
        "  ax1.set_title('Loss')\n",
        "  ax1.plot(history.history['loss'], label = 'train')\n",
        "  ax1.plot(history.history['val_loss'], label = 'test')\n",
        "  ax1.set_ylabel('Loss')\n",
        "\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "  # Plot accuracy\n",
        "  ax2.set_title('Accuracy')\n",
        "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
        "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE_Q0GbeH5jF"
      },
      "source": [
        "#### Question 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8dk4CSCdgj0"
      },
      "source": [
        "Why is there a gap between the accuracy on the training set and the accuracy on the validation set? Why does the training loss decrease while the validation loss stagnates?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-BNVLHGH5jG"
      },
      "source": [
        "---Fill your answer here---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code. This will be used for Question 3.4, but will also be used in other parts later."
      ],
      "metadata": {
        "id": "ugOTjUpwwcvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRUx5ao9QRcO"
      },
      "outputs": [],
      "source": [
        "y_hat = classifier.predict(x=x_val)\n",
        "y_hat = np.argmax(y_hat, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVidbr0OT5tL"
      },
      "outputs": [],
      "source": [
        "labels_dict = dict(zip(df_test['nine_box_category'], df_test['label']))\n",
        "labels_dict_reversed = dict(zip(df_test['label'], df_test['nine_box_category']))\n",
        "\n",
        "labels_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOgva0pbP4FS"
      },
      "source": [
        "#### Question 3.4 - Confusion Matrix [Only for students enrolled in the graduate version of the class]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeB8jJp2dgj1"
      },
      "source": [
        "Similarly to the classical classification [example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Classify_text_with_embeddings.ipynb#scrollTo=_mwJYXpElYJc), build and observe the confusion matrix, which characterize the model accuracy across the difference classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ae76701e178"
      },
      "outputs": [],
      "source": [
        "cm = skmetrics.confusion_matrix(y_val, y_hat)\n",
        "disp = skmetrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=labels_dict.keys())\n",
        "disp.plot(xticks_rotation='vertical')\n",
        "plt.title('Confusion matrix for newsgroup test dataset');\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY3LInttH5jK"
      },
      "source": [
        "What does the confusion matrix reveal about the model's performance? Are there any specific types of errors you can identify?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC4ZeAazH5jK"
      },
      "source": [
        "---Fill your answer here---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wj94u1FPHnu"
      },
      "source": [
        "## Question 4: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXXzC4jYdgj1"
      },
      "source": [
        "After looking at the quantitative evaluation of the model, now we will look at qualitative analysis. This will involve exploring the data and identifying some custom categories and labels. To start, let's add a new column to the dataset with the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaArNIvrPKPG"
      },
      "outputs": [],
      "source": [
        "y_hat_train = ## YOUR CODE TO CALL THE CLASSIFIER ON TRAIN DATA HERE\n",
        "y_hat_train = np.argmax(y_hat_train, axis=1)\n",
        "df_train['predicted_label'] = y_hat_train\n",
        "\n",
        "y_hat_test = ## YOUR CODE TO CALL THE CLASSIFIER ON TEST DATA HERE\n",
        "y_hat_test = np.argmax(y_hat_test, axis=1)\n",
        "df_test['predicted_label'] = y_hat_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hqm0Z3AU8zG"
      },
      "source": [
        "#### Question 4.1: Custom Ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nTIP1VRdgj2"
      },
      "source": [
        "In multiple cases (and also in our dataset), some of the annotated labels might be noisy or incorrect. To that end, we would like to utilize the classifier we have developed and a human annotator to re-generate some labels. Please implement a short function that lets the user decide on the right label, and records the user choice as the new label. Then, use this function to extend the dataset and create a new set of labels. Your loop should display the confidence interval of model accuracy measured against the new labels.\n",
        "\n",
        "\n",
        "You can label as many as you want, but you should label at least 30 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oho05yGvV4_j"
      },
      "outputs": [],
      "source": [
        "# Function to manually label each example\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def manual_labeling(row):\n",
        "    \"\"\"\n",
        "    Function to manually label data. Shows the data for review, collects the\n",
        "    updated response, and then return either the original label or the updated\n",
        "    label, depending on the response.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Feedback is: {row[['feedback']]}\")\n",
        "    print(f\"Original Label: {labels_dict_reversed[row['label']]}\")\n",
        "    print(f\"Predicted Label: {labels_dict_reversed[row['predicted_label']]}\")\n",
        "\n",
        "    # Ask the user for manual input on which label is correct\n",
        "    correct_label = ## YOUR CODE HERE\n",
        "\n",
        "    ## Subtract 1 to deal with 0 indexing\n",
        "    return (int(correct_label) - 1)\n",
        "\n",
        "# Apply the manual labeling function to each row and store the results\n",
        "examples_to_inspect = 30\n",
        "\n",
        "# Create new colums with default values of -1 to indicate they are\n",
        "# unlabeled\n",
        "df_train['manual_label'] = -1\n",
        "df_test['manual_label'] = -1\n",
        "for i in range(len(df_train)):\n",
        "\n",
        "  # Apply manual labeling for df_train\n",
        "  df_train.iloc[i, df_train.columns.get_loc('manual_label')] = manual_labeling(\n",
        "      df_train.iloc[i])\n",
        "\n",
        "  # Apply manual labeling for df_test\n",
        "  df_test.iloc[i, df_test.columns.get_loc('manual_label')] = manual_labeling(\n",
        "      df_test.iloc[i])\n",
        "\n",
        "  # calculate the new confidence intervals width: this is the same as before,\n",
        "  # but with the updated labels\n",
        "  Curr_CI_onesided_width = ## YOUR CODE HERE\n",
        "\n",
        "  # Print the current CI every 10 examples\n",
        "  if i%10 == 0:\n",
        "    print('=======================================================')\n",
        "    print(f\"=========Current i is {i}\")\n",
        "    print(f\"Current CI: {Curr_CI_onesided_width}\")\n",
        "    print(f\"Original CI: {initial_CI_onesided_width}\")\n",
        "\n",
        "  if i >= examples_to_inspect:\n",
        "      break\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"\\nUpdated DataFrame with manual labels:\")\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txAXZsdcYgoP"
      },
      "source": [
        "#### Question 4.2 Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBg8Z6V3dgj2"
      },
      "source": [
        "How well do your labels agree with the labels in the dataset? Are there any patterns to the differences? How might a model trained on the new labels be different?\n",
        "\n",
        "Please write 3-5 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k56b6kgJYt4d"
      },
      "source": [
        "---Fill your answer here---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HFFuzEYr7Ug"
      },
      "source": [
        "## Question 5: Creating New Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6SbMcfr7Ug"
      },
      "source": [
        "So far, over the course of the assignment, we have moved to interact more and more with the actual data:\n",
        "\n",
        "1.   First, you looked at the data documentation and information about the dataset.\n",
        "2.   Second, you looked an quantitative evaluation of a model using the existing labels.\n",
        "3.   Third, you did a manual subjective evaluation of the model using your own interpretation of the labels.\n",
        "\n",
        "Now, we'll go one step further and invent some new categories for the data. The first step is to do initial coding of the data. For this step, we will collect some free form annotations of the data.\n",
        "\n",
        "*Hint*: If you're not sure how to go about annotating data, look at the content in Ch. 5 of Charmaz about initial coding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwjfT8lYr7Ug"
      },
      "source": [
        "### Question 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivek8l_3dgj2"
      },
      "source": [
        "First, modify the code below to collect annotations of the data and store them in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgh1VhB5r7Ug"
      },
      "outputs": [],
      "source": [
        "## Add new columns to hold the annotations. Initialize with an empty string\n",
        "\n",
        "df_train['annotation'] = ''\n",
        "df_test['annotation'] = ''\n",
        "\n",
        "def annotate(row):\n",
        "    print(f\"Feedback is: {row['feedback']}\")\n",
        "    print(f\"Original Label: {labels_dict_reversed[row['label']]}\")\n",
        "    print(f\"Predicted Label: {labels_dict_reversed[row['predicted_label']]}\")\n",
        "\n",
        "    # Ask the user for an annotation for the example\n",
        "    annotation = ## YOUR CODE HERE\n",
        "\n",
        "    # Return the annotation\n",
        "    return annotation\n",
        "\n",
        "# Apply the function to your dataset\n",
        "# Apply the manual labeling function to each row and store the results\n",
        "examples_to_inspect = 4\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "\n",
        "  # Annotate an example from the training set\n",
        "  df_train.iloc[i, df_train.columns.get_loc('annotation')] = annotate(\n",
        "      df_train.iloc[i])\n",
        "\n",
        "  # Annotate an example from the testing set\n",
        "  df_test.iloc[i, df_test.columns.get_loc('annotation')] = annotate(\n",
        "      df_test.iloc[i])\n",
        "\n",
        "  if i >= examples_to_inspect:\n",
        "      break\n",
        "\n",
        "  # Print the current iteration index\n",
        "  if i%3 == 0:\n",
        "    print('=======================================================')\n",
        "    print(f\"=========Current i is {i}=========\")\n",
        "\n",
        "# Display the collected Annotations\n",
        "for _, row in df_train.iterrows():\n",
        "  if row['annotation'] != '':\n",
        "    print(f\"Feedback is: {row['feedback']}\")\n",
        "    print(f\"Original Label: {labels_dict_reversed[row['label']]}\")\n",
        "    print(f\"Predicted Label: {labels_dict_reversed[row['predicted_label']]}\")\n",
        "    print(f\"Annotation: {row['annotation']}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5kfijw7qArS"
      },
      "source": [
        "### Question 5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGoNXu0ldgj2"
      },
      "source": [
        "Now that we have our annotations, the next step is to move from initial coding to focused coding. Review your annotations and identify 3 interesting similarities or differences that you found in the data. Next propose 2 different ways of categorizing the data based on the similarities.\n",
        "\n",
        "For example, your annotations may have noticed that some reviews focus on interpersonal behavior (e.g., how well the employee works on a team) while others focus on work quality (e.g., do they finish their work on time and in a quality manner). In this case, you might create a category called \"Review Focus\" with labels: \"other\", \"interpersonal skills\", \"work quality\".\n",
        "\n",
        "For each category please:\n",
        "\n",
        "1.   Provide a 1-2 sentence summary.\n",
        "2.   List the labels in that category.\n",
        "3.   Provide an example of each label.\n",
        "4.   Describe why the categorization might be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68az87I7W3dX"
      },
      "outputs": [],
      "source": [
        "## Create code that defines your categories\n",
        "## Maps category name to (labels, summary, examples)\n",
        "\n",
        "custom_categories = {'CUSTOM CATEGORY 1': (('LABEL 1', 'LABEL 2'),\n",
        "                                           \"CATEGORY 1 SUMMARY\"),\n",
        "                     'CUSTOM CATEGORY 2': (('LABEL 1', 'LABEL 2'),\n",
        "                                           \"CATEGORY 2 SUMMARY\")}\n",
        "\n",
        "## Create a dictionary that maps (Category, Label) -> Example\n",
        "example_dict = {('CATEGORY', 'LABEL'): 'EXAMPLE FROM ANNOTATIONS'}\n",
        "\n",
        "\n",
        "for category in custom_categories:\n",
        "  print(f\"Category is: {category}; {custom_categories[category][1]}\")\n",
        "  print(f\"Labels are: {list(enumerate(custom_categories[category][0]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrnp4ldz92Rm"
      },
      "source": [
        "#### In 3-5 sentences, explain why each category might be useful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS9xXk7HqiGH"
      },
      "source": [
        "---Fill your answer here---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg_JsrV5Y-p_"
      },
      "source": [
        "Now we can add a new column to the dataset for each category. We will use -1 to denote unlabeled entries and the order of the labels in the dictionaries above to map numbers to labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6DMaEPXZXW-"
      },
      "outputs": [],
      "source": [
        "for category in custom_categories:\n",
        "  df_train[category] = -1\n",
        "  df_test[category] = -1\n",
        "\n",
        "def custom_label(row, category, labels, summary):\n",
        "\n",
        "  print(f\"Feedback is: {row['feedback']}\")\n",
        "  print(f\"Original Label: {labels_dict_reversed[row['label']]}\")\n",
        "  print(f\"summary for category is: {summary}\")\n",
        "\n",
        "  # Ask the user for a label\n",
        "  label = ## YOUR CODE HERE\n",
        "\n",
        "  # Return the label\n",
        "  return labels[int(label)]\n",
        "\n",
        "# Apply the function to your dataset\n",
        "examples_to_inspect = 20\n",
        "\n",
        "def label_category(category, labels):\n",
        "\n",
        "  print(f\"Category is: {category}; {custom_categories[category][1]}\")\n",
        "  print(f\"Labels are: {list(enumerate(labels))}\")\n",
        "  for label in labels:\n",
        "    print(f\"Example for {label} is: {example_dict[category, label]}\")\n",
        "\n",
        "  print('\\n')\n",
        "  print('=======================================================')\n",
        "  print('\\n')\n",
        "\n",
        "  for i in range(len(df_train)):\n",
        "\n",
        "    # Annotate an example from the training set\n",
        "    df_train.iloc[i, df_train.columns.get_loc(category)]= custom_label(\n",
        "        df_train.iloc[i],category,labels,custom_categories[category][1])\n",
        "\n",
        "    # Annotate an example from the testing set\n",
        "    df_test.iloc[i, df_test.columns.get_loc(category)]= custom_label(\n",
        "        df_test.iloc[i],category,labels,custom_categories[category][1])\n",
        "\n",
        "    if i >= examples_to_inspect:\n",
        "        break\n",
        "\n",
        "    # Print the current iteration index\n",
        "    if i%3 == 0:\n",
        "      print('=======================================================')\n",
        "      print(f\"=========Current i is {i}=========\")\n",
        "\n",
        "# Label each category\n",
        "for category in custom_categories:\n",
        "  print('=======================================================')\n",
        "  label_category(category, labels = custom_categories[category][0])\n",
        "  print('=======================================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhyUWBzdgj4"
      },
      "source": [
        "Print the df_train head and see the additional columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd_fwv78HUcd"
      },
      "outputs": [],
      "source": [
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npycFahr7Uh"
      },
      "source": [
        "## Question 6: Building a Model for Your Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HldmZbk3dgj4"
      },
      "source": [
        "Now that we have some labels here, we will experiment with some methods for predicting your custom labels. One way to approach this would be to use the same method we did up above. If you'd like, you can implement that method and evaluate the new model quantiatively. However, you will likely find that performance is quite poor --- we tend to need more labels than is educationally productive to assign in a PSET!\n",
        "\n",
        "An alternative is to turn our classification problem into a text completion task. In practice, this involves phrasing the classification task in natural language (i.e., English for our purposes here). For instance, if we wanted to classify whether sentence \"I'm really disappointed that summer is over.\" is happy or sad, we could ask a language model to complete the following sentence:\n",
        "\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "Is the following sentence happy?. Answer with \"happy\", \"sad\", or \"neither\". \"I'm really disappointed that summer is over.\"\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "While it isn't gauranteed that this will work, in practice these types of methods can work well with 0 additional data --- as long as they are evaluated properly. While this might seem like magic (and it is amazing), it's always a good idea to be skeptical of what seems like a free lunch. There are some important caveats that we need to consider:\n",
        "\n",
        "*   The way you phrase your question directly determines the categories the model will recognize. Everything else will be influenced by the model's inherent biases. This isn't necessarily bad (for example, the model's bias towards generating coherent responses is crucial), but it means the model will rely on its data-driven interpretation of your prompt, which may or may not align with your intended meaning.\n",
        "\n",
        "*  Small changes to prompts can change lead to large changes in behavior. [For example](https://arxiv.org/abs/2309.03409), including the phrase \"Take a deep breath and work on this problem step-by-step\" or offering to \"pay\" the model for better answers has improved performance in experiments. The process of iterating on these *prompts* has gotten the name *prompt engineering*. It's more of an iterative design exercise then a science, but [best practices](https://llama.meta.com/docs/how-to-guides/prompting/) are starting to emerge.\n",
        "\n",
        "#### Optional Reading\n",
        "While the idea of reducing one problem to another that you already know how to solve is one of the fundamentals in computer science, the application of this to large language models doing text completion tasks was introduced in \"[Language Models are Unsupervised Multitask Learners.](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWWXxB2Bdgj4"
      },
      "source": [
        "### Question 6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3rWIi8dgj4"
      },
      "source": [
        "Write out 3 potential prompts for each custom category you created. Explain your reasoning for the prompt in 1-3 sentences each. The prompts should be phrased as a question that the model can complete, aim to classify between the classes you created.\n",
        "\n",
        "For example, if you had a category called \"Work Quality\" with labels \"bad\", \"moderately bad\", \"moderately good\" and \"good worker\", an example of a prompt can be: \"Please classify the work quality of this employee according to the next list- bad, moderately bad, moderately good or good worker: {}\"\n",
        "and where the {} is replaced by the sentence you want to classify, which in our case is the feedback field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DWxxPTDTM47"
      },
      "outputs": [],
      "source": [
        "prompts = {'CUSTOM CATEGORY 1': ['YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 'YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 'YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 ],\n",
        "           'CUSTOM CATEGORY 2': ['YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 'YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 'YOUR PROMPT HERE',\n",
        "                                 ## WHY MIGHT THIS PROMPT WORK?\n",
        "                                 ],}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJfMsnHJTI1x"
      },
      "source": [
        "Now that we have our prompts, we can evaluate them! We'll use the DistilBART language model, applying the designed prompts to the feedback field from the employees.\n",
        "\n",
        "For example, consider an interaction with the model. Suppose the prompt you designed is: 'Does the following employee have good potential? Answer with \"no\", \"mild potential\", or \"extraordinary potential\",' and the feedback field is: 'John's performance was poor in the last quarter, although he graduated first in his class at MIT and scored an A+ in the AI, Decision-Making, and Society course.' In this case, we expect the model to return 'extraordinary potential'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3azqBEkrr7Uh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# To accomplish that task, we will use the Bart model\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"valhalla/distilbart-mnli-12-1\", device = device)\n",
        "\n",
        "# Function to classify feedback based on prompt\n",
        "def classify_feedback(feedback, prompt, labels):\n",
        "    # Combine prompt with feedback\n",
        "    combined_input = # --- fill your code here --- #\n",
        "\n",
        "    # Perform zero-shot classification\n",
        "    classification_result = classifier(combined_input, candidate_labels=labels)\n",
        "\n",
        "    # Return the label with the highest score\n",
        "    return classification_result['labels'][0]\n",
        "\n",
        "# Apply summarization prompt\n",
        "# Apply the prompts to each feedback and store the generated results\n",
        "number_to_classify = 5\n",
        "df_train['category_0_prompt_0'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[0]][0],\n",
        "                      custom_categories[list(custom_categories.keys())[0]][0]))\n",
        "df_train['category_0_prompt_1'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[0]][1],\n",
        "                      custom_categories[list(custom_categories.keys())[0]][0]))\n",
        "df_train['category_0_prompt_2'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[0]][2],\n",
        "                      custom_categories[list(custom_categories.keys())[0]][0]))\n",
        "\n",
        "df_train['category_1_prompt_0'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[1]][0],\n",
        "                      custom_categories[list(custom_categories.keys())[1]][0]))\n",
        "df_train['category_1_prompt_1'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[1]][1],\n",
        "                      custom_categories[list(custom_categories.keys())[1]][0]))\n",
        "df_train['category_1_prompt_2'] = df_train.head(number_to_classify)['feedback']\\\n",
        "  .apply(lambda feedback: classify_feedback(feedback,\n",
        "                      prompts[list(custom_categories.keys())[1]][2],\n",
        "                      custom_categories[list(custom_categories.keys())[1]][0]))\n",
        "\n",
        "# print the results\n",
        "print(df_train[['feedback', list(custom_categories.keys())[0],\n",
        "                'category_0_prompt_0']].head(number_to_classify))\n",
        "print(df_train[['feedback', list(custom_categories.keys())[0],\n",
        "                'category_0_prompt_1']].head(number_to_classify))\n",
        "print(df_train[['feedback', list(custom_categories.keys())[0],\n",
        "                'category_0_prompt_2']].head(number_to_classify))\n",
        "\n",
        "print(df_train[['feedback', list(custom_categories.keys())[1],\n",
        "                'category_1_prompt_0']].head(number_to_classify))\n",
        "print(df_train[['feedback', list(custom_categories.keys())[1],\n",
        "                'category_1_prompt_1']].head(number_to_classify))\n",
        "print(df_train[['feedback', list(custom_categories.keys())[1],\n",
        "                'category_1_prompt_2']].head(number_to_classify))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3v-ZxpwmbS0"
      },
      "source": [
        "### Question 6.2 - Few-Shot Prompting [Only for students enrolled in the graduate version of the class]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35aZvboIdgj5"
      },
      "source": [
        "We now aim to perform few-shot prompting, where we use our zero-shot classifier (from the previous question) to create prompts that guide the model to classify unseen labels. Specifically, the model will use a few labeled examples in the prompt to enhance its understanding of new labels. Additionally, we would like to implement a function that re-generates prompts based on the predictions made by the model on the previous prompts, incorporating them into future examples to improve the prompting mechanism iteratively.\n",
        "\n",
        "Given the previous prompts and the model's predictions, check the response of the model on some new unseen prompts and labels. You can use the previous prompts and labels to generate the new prompts and labels, thus \"guiding\" the model towards the right answer by giving him the previously generated information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW1v5AqgrXLE"
      },
      "outputs": [],
      "source": [
        "#--- Fill your code here ---#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqkaWb_H5jO"
      },
      "source": [
        "## Question 7: Reflections on the overall process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmLMon4zdgj5"
      },
      "source": [
        "1. In your own words, describe the steps we used to build and evaluate models for employee review. What additional steps would you recommend for a company thinking of deploying such a model?\n",
        "\n",
        "2. How did your re-labeling change the performance? What does this indicate about the \"quality\" of the initial labels?\n",
        "\n",
        "3. If a company were to deploy such a model for employment review purposes, what are some pros and cons of using your custom categorization of the data compared to the original labels?\n",
        "\n",
        "4. What is the key idea behind using a language model for \"zero-shot classification\"? Did certain prompts work better than others, and why do you think that was the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnL7zPRydgj5"
      },
      "source": [
        "---Fill your answers here---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKZUuuiyH5jO"
      },
      "source": [
        "### For graduate students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK1QNjAQdgj5"
      },
      "source": [
        "5. How did few-shot prompting change the performance? What strategies did you find useful for choosing the few-shot examples?  \n",
        "\n",
        "---Fill your answer here---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "google": {
      "image_path": "/examples/train_text_classifier_embeddings_files/output_3ae76701e178_0.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}