# Exploratory Data Analysis and Data Curation in AI Decision-Making

---
    
## Introduction
    
In the field of artificial intelligence (AI), the quality of data plays a pivotal role in shaping model performance and reliability. Exploratory Data Analysis (EDA) and data curation are fundamental processes that ensure datasets are well-understood, clean, and suitable for training robust machine learning models. This project delves into the hands-on experience of EDA and data curation using the Employee Review dataset. By systematically investigating the dataset, training machine learning models, and evaluating their performance, this project highlights the critical steps involved in preparing data for AI applications. Additionally, it employs a minimal version of the Grounded Theory approach to define custom categories, emphasizing the impact of design decisions on model outcomes.

---
    
## Project Narrative
    
The primary objective of this project was to gain practical experience in exploratory data analysis and data curation within the context of AI-driven decision-making. Utilizing the Employee Review dataset, which comprises 880 employee performance reviews rated on a scale from 1 to 9, the project aimed to explore how AI systems can be trained and evaluated using real-world data. The dataset, sourced from Amazon MTurk workers, presents a unique opportunity to assess AI models' ability to interpret and classify employee performance and potential based on textual feedback.
    
The project commenced with setting up the computational environment, ensuring the necessary libraries and tools were in place for data manipulation and model training. Following this, the dataset was thoroughly examined to understand its structure, origin, and inherent biases. Initial analyses revealed that the dataset was partially reviewed, with only about 70% of records verified for consistency between feedback content and assigned categories. This partial review highlighted potential areas where model predictions could be influenced by mislabeled or inconsistent data.
    
Subsequent phases involved training a simple classifier using the dataset's embeddings and evaluating its performance. Despite achieving a reasonable accuracy on the validation set, a noticeable gap between training and validation performance indicated potential overfitting. To address this, the project incorporated manual labeling of a subset of data to refine the model's predictions and better understand the quality of the initial labels. This manual intervention underscored the importance of high-quality annotations in enhancing model reliability and fairness.
    
Further exploration led to the creation of custom categories, moving beyond the original labels to capture more nuanced aspects of employee performance and potential. By defining categories such as "Work Ethic and Discipline" and "Potential vs. Performance," the project aimed to provide a more granular analysis of employee reviews. This categorization facilitated a deeper understanding of the data, allowing for more targeted model training and evaluation.
    
---
    
## Approach and Methodology
    
### 1. Environment Setup and Data Loading
    
The project began by setting up a Python environment within Google Colab, leveraging its GPU capabilities to expedite model training processes. Essential libraries such as `pandas`, `numpy`, `keras`, `seaborn`, and `matplotlib` were installed to facilitate data manipulation, visualization, and machine learning tasks. The Employee Review dataset was then imported into the Colab environment, with separate CSV files for training and testing sets.
    
### 2. Data Examination and Documentation Review
    
A critical step involved reviewing the dataset's documentation to understand its construction, original purpose, and potential limitations. Key insights included:
    
- **Construction:** The dataset was generated by Amazon MTurk workers tasked with creating fake employee performance reviews based on a "9-box" model. Approximately 70% of the feedback was manually reviewed to ensure consistency.
- **Purpose:** Intended for research in sentiment analysis and to compare deep learning models with traditional models.
- **Limitations:** Partial review of data may lead to mislabeled entries, affecting model accuracy and introducing biases.
    
Potential applications were identified, highlighting appropriate uses such as training sentiment analysis models for HR purposes, and inappropriate uses like making automated hiring decisions without further validation.
    
### 3. Exploratory Data Analysis (EDA) and Data Curation
    
EDA involved loading the dataset into pandas DataFrames and inspecting the first few entries to grasp the data's structure and content. Initial observations pointed out that the feedback texts were diverse yet partially inconsistent due to the partial review process. To address this, manual cleaning and modifications were performed to enhance data quality. This included handling missing values, correcting mislabeled entries, and standardizing text formats.
    
### 4. Model Training and Quantitative Evaluation
    
A simple neural network classifier was constructed using Keras, comprising two fully-connected layers. The model was trained on embeddings generated from the feedback texts using the `embedding-001` model. Training parameters were set with 40 epochs and a batch size of 8, balancing training duration with computational efficiency.
    
The model's performance was evaluated using both training and validation sets. While the training accuracy was high, a significant gap between training and validation accuracy indicated overfitting. To mitigate this, manual labeling of a subset of the data was undertaken, refining the training process and enhancing the model's generalizability.
    
### 5. Custom Categorization and Qualitative Analysis
    
Building on the initial quantitative evaluation, custom categories were introduced to capture more nuanced aspects of employee feedback. Categories such as "Work Ethic and Discipline" and "Potential vs. Performance" were defined, each with specific labels and examples. This categorization aimed to provide a deeper, more targeted analysis of employee reviews, moving beyond the original labels to better reflect real-world performance and potential assessments.
    
Human annotators reviewed and annotated a subset of the data, ensuring that the model's predictions aligned with human interpretations. This dual approach of automated and manual annotation facilitated a more comprehensive evaluation of model performance, highlighting areas where the model excelled and where it required further refinement.
    
---
    
## Results
    
### 1. Model Performance
    
The initial classifier achieved a validation accuracy of approximately 70%, with a noticeable discrepancy between training and validation accuracy, signaling overfitting. Through manual labeling of 30 examples, the model's predictions were refined, resulting in a slight improvement in validation accuracy and a reduction in the confidence interval width. This indicated that enhancing label quality positively impacted model reliability and performance.
    
### 2. Bias Detection and Mitigation
    
The evaluation revealed that the model exhibited biases in predicting performance and potential, influenced by the quality and consistency of the initial labels. Manual re-labeling helped in identifying and correcting these biases, leading to more accurate and fair predictions. The introduction of custom categories allowed for a more detailed analysis, uncovering specific areas where the model struggled, such as distinguishing between "High Potential, Low Performance" and "Low Potential, Low Performance" categories.
    
### 3. Custom Categorization Insights
    
The creation of custom categories provided valuable insights into the data. For instance:
    
- **Work Ethic and Discipline:** Enabled the model to differentiate between employees who were disciplined versus those who were not, based on feedback.
- **Potential vs. Performance:** Allowed the identification of employees with high potential despite low current performance, and those with low potential and performance.
    
These categories facilitated a more granular understanding of employee feedback, enhancing the model's ability to make nuanced predictions. The visualization of bias distribution across these categories highlighted significant disparities, emphasizing the need for ongoing bias mitigation efforts.
    
### 4. Benchmarking and Real-world Applicability
    
Applying the trained model to real-world scenarios, such as automated employee evaluations, demonstrated the importance of high-quality data and robust evaluation frameworks. The findings underscored that even simple models could perpetuate biases if trained on inconsistent or biased data, reinforcing the necessity for comprehensive data curation and continuous model assessment.
    
---
    
## Reflections
    
This project provided a comprehensive journey through the stages of exploratory data analysis, data curation, model training, and evaluation within the context of AI-driven decision-making. One of the key takeaways was the profound impact of data quality on model performance. The initial discrepancies between training and validation accuracy highlighted how overfitting can mask underlying issues in data consistency and labeling. Manual labeling proved invaluable in uncovering and addressing these issues, reinforcing the importance of human oversight in AI evaluations.
    
The introduction of custom categories illuminated the limitations of predefined labels, showcasing the need for more tailored and context-specific categorizations to capture the complexities of real-world data. This approach not only enhanced model accuracy but also provided deeper insights into the data, facilitating more informed decision-making.
    
Moreover, the project underscored the ethical implications of deploying AI models in sensitive areas like employee evaluations. Biases in model predictions could have significant repercussions, reinforcing societal inequalities and undermining trust in AI systems. This realization emphasizes the critical need for robust evaluation frameworks, continuous monitoring, and proactive bias mitigation strategies to ensure that AI technologies serve all segments of society equitably.
    
Overall, the project was both challenging and enlightening, bridging theoretical knowledge with practical application. It reinforced the importance of meticulous data curation and thoughtful model evaluation in developing reliable and fair AI systems.

---
    
## Conclusion and Learnings
    
Through this project, the intricate relationship between data quality, model performance, and ethical AI deployment was thoroughly explored. The hands-on experience in EDA and data curation highlighted the necessity of understanding and refining datasets to eliminate biases and ensure accurate model predictions. The iterative process of model training, evaluation, and manual labeling demonstrated the importance of human intervention in enhancing AI reliability and fairness.
    
The development of custom categories provided a deeper, more nuanced analysis of employee feedback, showcasing how tailored categorizations can improve model interpretability and performance. This approach is particularly relevant in real-world applications where predefined labels may not capture the full spectrum of relevant factors.
    
Additionally, the project emphasized the ethical responsibilities inherent in deploying AI systems, especially in areas impacting individuals' careers and livelihoods. Ensuring that AI models are free from biases and operate transparently is crucial for fostering trust and promoting equitable outcomes.
    
Overall, the project reinforced the significance of comprehensive data exploration, meticulous curation, and continuous model evaluation in the responsible development and deployment of AI technologies.

---
    
## Skills Demonstrated
    
- **Exploratory Data Analysis (EDA):** Conducted thorough examinations of datasets to understand structure, content, and inherent biases.
- **Data Curation:** Performed data cleaning, handling missing values, and refining labels to enhance dataset quality.
- **Machine Learning Model Training:** Built and trained neural network classifiers using Keras, leveraging embeddings for performance prediction.
- **Quantitative Evaluation:** Assessed model performance using accuracy metrics and confidence intervals, identifying overfitting issues.
- **Qualitative Analysis:** Developed custom categories to capture nuanced aspects of employee performance and potential.
- **Bias Detection and Mitigation:** Identified and addressed biases in model predictions through manual labeling and data refinement.
- **Human-AI Collaboration:** Combined automated and manual annotation methods to improve model reliability and fairness.
- **Technical Documentation:** Documented processes, methodologies, and findings comprehensively for clear communication.
- **Critical Thinking:** Evaluated the impact of data quality on model performance and the ethical implications of AI deployment.
- **Problem-Solving:** Addressed challenges related to overfitting and biased predictions through iterative evaluation and data curation.

---
    
## Project Links
    
- **GitHub Repository:** [EDA-Data-Curation-Project](https://github.com/yourusername/EDA-Data-Curation-Project)
- **Colab Notebook:** [Exploratory Data Analysis Notebook](https://colab.research.google.com/drive/1dMFF4m75QJvoybdaBJx8wlKfsoWVpY49?usp=sharing)
- **Chat Transcripts:**
  - **Data Examination and Cleaning:**
    - [Data Documentation Review](https://chatgpt.com/share/your_data_doc_review_chat_link)
  - **Model Training and Evaluation:**
    - [Model Performance Discussion](https://chatgpt.com/share/your_model_performance_chat_link)
  - **Custom Categorization and Annotation:**
    - [Custom Categories Annotation](https://chatgpt.com/share/your_custom_categories_chat_link)
    
---
    
## Visuals
    
- **Model Performance Over Epochs:**  
  ![Model Performance](https://yourimagehost.com/model_performance.png)  
  *This graph illustrates the training and validation accuracy and loss over 40 epochs, highlighting signs of overfitting.*
    
- **Bias Distribution in Custom Categories:**  
  ![Bias Distribution](https://yourimagehost.com/bias_distribution.png)  
  *Visualization of bias labels across "Work Ethic and Discipline" and "Potential vs. Performance" categories, showcasing disparities in model predictions.*
    
---
    
## Additional Reflections
    
Engaging in this project provided profound insights into the pivotal role of data quality in AI-driven decision-making. The iterative process of exploratory data analysis, combined with hands-on model training and evaluation, underscored the complexities involved in developing reliable and fair machine learning models. One of the most striking realizations was how easily models can perpetuate existing biases present in the data, emphasizing the ethical responsibilities of AI practitioners to ensure fairness and accountability.
    
The introduction of custom categories was particularly enlightening, demonstrating how tailored data annotations can significantly enhance model interpretability and performance. This approach is invaluable in real-world applications where predefined labels may not capture the full complexity of the data. Additionally, the collaboration between automated and manual annotation processes highlighted the importance of human oversight in refining AI models, ensuring that they align with nuanced human interpretations and ethical standards.
    
Overall, this project reinforced the necessity of meticulous data curation and continuous model evaluation in the responsible development of AI systems. It has equipped me with the skills and understanding necessary to contribute to creating more equitable and trustworthy AI technologies, fostering positive societal impacts.

---
